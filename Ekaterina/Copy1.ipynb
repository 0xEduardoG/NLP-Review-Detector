{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d827980d-0c09-4b46-8cb0-3c9fc5f6f4dd",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533803b6-91ec-4faa-a57e-4ca7192dc1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, recall_score,\\\n",
    "    accuracy_score, precision_score, f1_score, classification_report\n",
    "import warnings\n",
    "import xgboost\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be5cb17-f005-465d-939b-f2babbb063b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                               text_  \n",
       "0  Love this!  Well made, sturdy, and very comfor...  \n",
       "1  love it, a great upgrade from the original.  I...  \n",
       "2  This pillow saved my back. I love the look and...  \n",
       "3  Missing information on how to use it, but it i...  \n",
       "4  Very nice set. Good quality. We have had the s...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fake reviews dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed7da3-dfe0-4207-b8f2-45d871c728a2",
   "metadata": {},
   "source": [
    "## Data Undersatding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ea703a-88c1-4ccb-b6a4-9376358321d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Kindle_Store_5                  4730\n",
       "Books_5                         4370\n",
       "Pet_Supplies_5                  4254\n",
       "Home_and_Kitchen_5              4056\n",
       "Electronics_5                   3988\n",
       "Sports_and_Outdoors_5           3946\n",
       "Tools_and_Home_Improvement_5    3858\n",
       "Clothing_Shoes_and_Jewelry_5    3848\n",
       "Toys_and_Games_5                3794\n",
       "Movies_and_TV_5                 3588\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034dc82e-b462-48fd-b1f2-5438d3fa94d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40432.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.256579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.144354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rating\n",
       "count  40432.000000\n",
       "mean       4.256579\n",
       "std        1.144354\n",
       "min        1.000000\n",
       "25%        4.000000\n",
       "50%        5.000000\n",
       "75%        5.000000\n",
       "max        5.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab91030-1d16-4cce-8656-920c83fab120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 'target' Column for Classification\n",
    "df['target'] = np.where(df['label'] == 'CG', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46dbda20-13ca-47a2-aa10-62d1b0379121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I had read some reviews saying that this bra r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I wasn't sure exactly what it would be. It is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>You can wear the hood by itself, wear it with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I liked nothing about this dress. The only rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I work in the wedding industry and have to wor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           category  rating label  \\\n",
       "0                Home_and_Kitchen_5     5.0    CG   \n",
       "1                Home_and_Kitchen_5     5.0    CG   \n",
       "2                Home_and_Kitchen_5     5.0    CG   \n",
       "3                Home_and_Kitchen_5     1.0    CG   \n",
       "4                Home_and_Kitchen_5     5.0    CG   \n",
       "...                             ...     ...   ...   \n",
       "40427  Clothing_Shoes_and_Jewelry_5     4.0    OR   \n",
       "40428  Clothing_Shoes_and_Jewelry_5     5.0    CG   \n",
       "40429  Clothing_Shoes_and_Jewelry_5     2.0    OR   \n",
       "40430  Clothing_Shoes_and_Jewelry_5     1.0    CG   \n",
       "40431  Clothing_Shoes_and_Jewelry_5     5.0    OR   \n",
       "\n",
       "                                                   text_  target  \n",
       "0      Love this!  Well made, sturdy, and very comfor...       1  \n",
       "1      love it, a great upgrade from the original.  I...       1  \n",
       "2      This pillow saved my back. I love the look and...       1  \n",
       "3      Missing information on how to use it, but it i...       1  \n",
       "4      Very nice set. Good quality. We have had the s...       1  \n",
       "...                                                  ...     ...  \n",
       "40427  I had read some reviews saying that this bra r...       0  \n",
       "40428  I wasn't sure exactly what it would be. It is ...       1  \n",
       "40429  You can wear the hood by itself, wear it with ...       0  \n",
       "40430  I liked nothing about this dress. The only rea...       1  \n",
       "40431  I work in the wedding industry and have to wor...       0  \n",
       "\n",
       "[40432 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17583586-ab05-4182-9acc-1d5e645fff49",
   "metadata": {},
   "source": [
    "## Text Preprocessing: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d63507-d2a1-49ff-a5fc-879a191194c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the text data in the 'text_' column of df\n",
    "def tokenizer(x):\n",
    "    \n",
    "    corpus = [word_tokenize(doc) for doc in x]\n",
    "\n",
    "# Getting common stop words in english that we'll remove during tokenization/text normalization\n",
    "    stop_words = stopwords.words('english')\n",
    "    corpus_no_stopwords = []\n",
    "    for words in corpus:\n",
    "        docs = [x.lower() for x in words if ((x.isalpha()) & (x not in stop_words))]\n",
    "        corpus_no_stopwords.append(docs)\n",
    "    return corpus_no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c671ab9-1944-48ff-96db-c374083e0f0c",
   "metadata": {},
   "source": [
    "## Lemmantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8703adb6-185e-474d-8d4d-6e9d4c702340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining new function 'lemmatizer'. This function takes two arguments: corpus(list of sentences or text data that we want to lemmatize), and as_string \n",
    "def lemmatizer(corpus, as_string=True):\n",
    "    lem = WordNetLemmatizer()\n",
    "# Defining an inner function 'pos_tagger'    \n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "    lemmatized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        pos_tags = pos_tag(sentence)\n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in pos_tags:\n",
    "            pos = pos_tagger(tag)\n",
    "            if pos is not None:\n",
    "                lemmatized_word = lem.lemmatize(word, pos)\n",
    "            else:\n",
    "                lemmatized_word = lem.lemmatize(word)\n",
    "            lemmatized_sentence.append(lemmatized_word)\n",
    "        lemmatized_corpus.append(lemmatized_sentence)\n",
    "    if as_string:\n",
    "        lemmatized_corpus  = [' '.join(x) for x in lemmatized_corpus]\n",
    "    return lemmatized_corpus\n",
    "# After processing all words in the sentence, the lemmatized_sentence is added to the lemmatized_corpus\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1206737e-503f-4251-94e1-1f0539cc12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing a text corpus\n",
    "corpus_tokenized = tokenizer(df['text_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "747e8fba-94e9-4fe4-b8e2-e8a68e842d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing a text corpus\n",
    "lemmatized_corpus = lemmatizer(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a533175-9176-40cc-89f2-32503366188d",
   "metadata": {},
   "source": [
    "## Pre vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72435717-6b57-4004-96aa-9c3158368198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Preprocessed Text Column to DataFrame\n",
    "joined_lemm_corpus = [' '.join(x) for x in lemmatized_corpus]\n",
    "df['text_preproccesed'] = pd.Series(data=lemmatized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb776e9b-7c6d-4c1a-a273-25cf41a0289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Document-Term Matrix Using CountVectorizer\n",
    "vec = CountVectorizer(min_df = 0.05, max_df = 0.95)\n",
    "X = vec.fit_transform(lemmatized_corpus)\n",
    "countvec_df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03accb0c-d1d8-4ff2-a841-dc49b9704b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating TF-IDF Matrix Using TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df = 0.05, max_df = 0.95)\n",
    "Y = tfidf.fit_transform(lemmatized_corpus)\n",
    "tfidf_df = pd.DataFrame(Y.toarray(), columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14d7c444-29a6-4b89-80c6-d3c931eaf57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>anyone</th>\n",
       "      <th>best</th>\n",
       "      <th>big</th>\n",
       "      <th>bit</th>\n",
       "      <th>book</th>\n",
       "      <th>buy</th>\n",
       "      <th>ca</th>\n",
       "      <th>character</th>\n",
       "      <th>come</th>\n",
       "      <th>...</th>\n",
       "      <th>try</th>\n",
       "      <th>two</th>\n",
       "      <th>use</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115232</td>\n",
       "      <td>0.206830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061434</td>\n",
       "      <td>0.067497</td>\n",
       "      <td>0.252213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>0.064548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>0.179017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.110217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           also  anyone      best       big       bit  book      buy  \\\n",
       "0      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "1      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "2      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "3      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "4      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "...         ...     ...       ...       ...       ...   ...      ...   \n",
       "40427  0.000000     0.0  0.112657  0.000000  0.000000   0.0  0.00000   \n",
       "40428  0.000000     0.0  0.000000  0.091199  0.000000   0.0  0.13296   \n",
       "40429  0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "40430  0.064548     0.0  0.000000  0.000000  0.145785   0.0  0.11615   \n",
       "40431  0.179017     0.0  0.000000  0.000000  0.101079   0.0  0.00000   \n",
       "\n",
       "             ca  character      come  ...       try       two       use  \\\n",
       "0      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.436464   \n",
       "4      0.000000        0.0  0.000000  ...  0.000000  0.350925  0.000000   \n",
       "...         ...        ...       ...  ...       ...       ...       ...   \n",
       "40427  0.000000        0.0  0.103073  ...  0.115232  0.206830  0.000000   \n",
       "40428  0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "40429  0.000000        0.0  0.000000  ...  0.174608  0.000000  0.000000   \n",
       "40430  0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "40431  0.110217        0.0  0.000000  ...  0.000000  0.000000  0.151224   \n",
       "\n",
       "           want       way      well      work     would  write      year  \n",
       "0      0.000000  0.000000  0.365431  0.000000  0.000000    0.0  0.000000  \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.728227  \n",
       "2      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000  \n",
       "3      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000  \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000  \n",
       "...         ...       ...       ...       ...       ...    ...       ...  \n",
       "40427  0.099786  0.000000  0.075666  0.000000  0.077660    0.0  0.000000  \n",
       "40428  0.000000  0.000000  0.061434  0.067497  0.252213    0.0  0.000000  \n",
       "40429  0.000000  0.000000  0.114655  0.000000  0.117676    0.0  0.000000  \n",
       "40430  0.000000  0.000000  0.107335  0.000000  0.055082    0.0  0.000000  \n",
       "40431  0.000000  0.100946  0.000000  0.327058  0.000000    0.0  0.000000  \n",
       "\n",
       "[40432 rows x 89 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f42437c-3fb2-42df-b4b7-e087b539c05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40432x89 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 375538 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e476822-c81d-4753-8edb-19807b3f1b25",
   "metadata": {},
   "source": [
    "### Splitting Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b6bd1dd-042d-4720-a383-cf3879867a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(df['text_'],df['target'], test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53d46c6a-bc4b-4111-aa62-614373c80165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Text Data in Training and Testing Sets\n",
    "X_train_preprocessed = lemmatizer(tokenizer(X_train))\n",
    "X_test_preprocessed = lemmatizer(tokenizer(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22781fb0-adf4-4b27-a7c7-0acd8d3a9d82",
   "metadata": {},
   "source": [
    "### Random Forest and Logistic Regression with the Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62f4bc55-fb81-48a7-b673-5ced3b852b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Evaluation Metrics Function\n",
    "def scores(y_test, y_pred):\n",
    "    print(f\"\"\"    Accuracy: {round(accuracy_score(y_test,y_pred),3)}\n",
    "    \n",
    "    Recall: {round(recall_score(y_test,y_pred),3)}\n",
    "    \n",
    "    F1: {round(f1_score(y_test,y_pred),3)}\n",
    "    \n",
    "    Precision: {round(precision_score(y_test,y_pred),3)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ad343d0-0766-472b-8b1f-8f188d777c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9825100699597201"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and Evaluate Random Forest Model with CountVectorizer\n",
    "steps = [('countvec',CountVectorizer(min_df = 0.05, max_df = 0.95)),('rfc',RandomForestClassifier(n_estimators=200,random_state=42))]\n",
    "pipe_cv_rf = Pipeline(steps)\n",
    "pipe_cv_rf.fit(X_train_preprocessed, y_train)\n",
    "pipe_cv_rf.score(X_train_preprocessed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca08dad2-0781-406c-9582-087072645b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Mean Cross-Validation Score for Random Forest Model with CountVectorizer\n",
    "crossval_rf_cv = np.mean(cross_val_score(pipe_cv_rf, X_train_preprocessed,y_train,scoring='accuracy',cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db83b662-c8f3-4538-9af6-7082e87a8fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7627021346850904"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_rf_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "affd534b-5fa0-4e26-ade7-2123a2d59be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7288177513956611"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and Evaluate Logistic Regression Model with CountVectorizer\n",
    "steps[1] = ('logreg',LogisticRegression(random_state=42))\n",
    "pipe_cv_lr = Pipeline(steps)\n",
    "pipe_cv_lr.fit(X_train_preprocessed, y_train)\n",
    "pipe_cv_lr.score(X_train_preprocessed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56924eb5-9d2a-41d6-8762-0ac4d15f463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Mean Cross-Validation Score for Logistic Regression Model with CountVectorizer\n",
    "crossval_lr_cv = np.mean(cross_val_score(pipe_cv_lr, X_train_preprocessed,y_train,scoring='accuracy',cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57114d2c-1b4f-40af-8445-ffa5bbd47823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7266269054338063"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_lr_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861cfa49-37e2-462b-bb2a-79dd9f2f7276",
   "metadata": {},
   "source": [
    "### Random Forest and Logistic Regression with the Tfidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ac11e5e-a63f-4920-835f-5017cd2fe56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98187407250371"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and Evaluate Random Forest Model with TfidfVectorizer\n",
    "steps = [('tfidfvec',TfidfVectorizer(min_df = 0.05, max_df = 0.95)),('rfc',RandomForestClassifier(n_estimators=200,random_state=42))]\n",
    "pipe_idf_rf = Pipeline(steps)\n",
    "pipe_idf_rf.fit(X_train_preprocessed, y_train)\n",
    "pipe_idf_rf.score(X_train_preprocessed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23fb474b-c47a-47df-b660-bc71d2fe9075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Mean Cross-Validation Score for Random Forest Model with TfidfVectorizer\n",
    "crossval_rf_idf = np.mean(cross_val_score(pipe_idf_rf, X_train_preprocessed,y_train,scoring='accuracy',cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06c32541-4d60-4472-a80d-b910de1a0553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631969029932032"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_rf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56ae5c37-54cf-45b3-b73d-899aa1f12c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7300190799236803"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and Evaluate Logistic Regression Model with TfidfVectorizer\n",
    "steps[1] = ('logreg',LogisticRegression(random_state=42))\n",
    "pipe_idf_lr = Pipeline(steps)\n",
    "pipe_idf_lr.fit(X_train_preprocessed, y_train)\n",
    "pipe_idf_lr.score(X_train_preprocessed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80af72b9-242e-4f2f-a351-763d7c37d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Mean Cross-Validation Score for Logistic Regression Model with TfidfVectorizer\n",
    "crossval_lr_idf = np.mean(cross_val_score(pipe_idf_lr, X_train_preprocessed,y_train,scoring='accuracy',cv=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87185276-1818-4145-855f-cdeeffbc8708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7267683043675561"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_lr_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada7ea6d-814b-4450-a644-8bb6904658cb",
   "metadata": {},
   "source": [
    "- We run two models with both types of vectorizers, and based on our scores, we decided to stick with the Count Vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b32b2c25-d9e0-4737-8c63-e7ef1e850c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Pipelines for Random Forest and Logistic Regression Models with CountVectorizer\n",
    "steps_rf = [('countvec',CountVectorizer(min_df = 0.05, max_df = 0.95)),('rfc',RandomForestClassifier(n_estimators=200,random_state=42))]\n",
    "steps_lr = [('countvec',CountVectorizer(min_df = 0.05, max_df = 0.95)),('logreg',LogisticRegression(random_state=42,max_iter=10000))]\n",
    "\n",
    "pipe_rf = Pipeline(steps_rf)\n",
    "pipe_lr = Pipeline(steps_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1be4aa-f6a2-433b-9250-82562de59ee1",
   "metadata": {},
   "source": [
    "### Hyperparameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a048d452-1a7f-481c-a84e-4c0a4dc7e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Hyperparameter Grids for Random Forest and Logistic Regression Models\n",
    "param_rf = {'rfc__n_estimators':[100,150,200],\n",
    "                       'rfc__max_depth':[2,3,4,5],\n",
    "                       'rfc__min_samples_leaf':[1,2,3]\n",
    "                      }\n",
    "\n",
    "param_lr = {'logreg__penalty':['l1', 'l2', 'elasticnet', None],\n",
    "                 'logreg__C':[0.001,0.01,0.1,1,10,100],\n",
    "                 'logreg__solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1f445d-a0dc-4593-8963-008339e35e65",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7bc7c3ea-486d-4c68-a28b-7893f9545264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, error_score=0,\n",
       "             estimator=Pipeline(steps=[(&#x27;countvec&#x27;,\n",
       "                                        CountVectorizer(max_df=0.95,\n",
       "                                                        min_df=0.05)),\n",
       "                                       (&#x27;logreg&#x27;,\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           random_state=42))]),\n",
       "             param_grid={&#x27;logreg__C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;logreg__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;, None],\n",
       "                         &#x27;logreg__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;newton-cg&#x27;,\n",
       "                                            &#x27;newton-cholesky&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, error_score=0,\n",
       "             estimator=Pipeline(steps=[(&#x27;countvec&#x27;,\n",
       "                                        CountVectorizer(max_df=0.95,\n",
       "                                                        min_df=0.05)),\n",
       "                                       (&#x27;logreg&#x27;,\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           random_state=42))]),\n",
       "             param_grid={&#x27;logreg__C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;logreg__penalty&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;elasticnet&#x27;, None],\n",
       "                         &#x27;logreg__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;liblinear&#x27;, &#x27;newton-cg&#x27;,\n",
       "                                            &#x27;newton-cholesky&#x27;, &#x27;sag&#x27;, &#x27;saga&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;countvec&#x27;, CountVectorizer(max_df=0.95, min_df=0.05)),\n",
       "                (&#x27;logreg&#x27;,\n",
       "                 LogisticRegression(max_iter=10000, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=0.95, min_df=0.05)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, error_score=0,\n",
       "             estimator=Pipeline(steps=[('countvec',\n",
       "                                        CountVectorizer(max_df=0.95,\n",
       "                                                        min_df=0.05)),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           random_state=42))]),\n",
       "             param_grid={'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'logreg__penalty': ['l1', 'l2', 'elasticnet', None],\n",
       "                         'logreg__solver': ['lbfgs', 'liblinear', 'newton-cg',\n",
       "                                            'newton-cholesky', 'sag', 'saga']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_rf = GridSearchCV(estimator=pipe_rf, param_grid=param_rf,scoring='accuracy',cv=5,error_score=0)\n",
    "gs_lr = GridSearchCV(estimator=pipe_lr, param_grid=param_lr,scoring='accuracy',cv=5,error_score=0)\n",
    "gs_rf.fit(X_train_preprocessed,y_train)\n",
    "gs_lr.fit(X_train_preprocessed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0b14f3f-cc27-4185-bd9d-b22c276b386f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rfc__max_depth': 5, 'rfc__min_samples_leaf': 2, 'rfc__n_estimators': 100}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Hyperparameters for Random Forest Model\n",
    "gs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fb062ec-256a-4051-9349-3b7c7a2ab241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg__C': 1, 'logreg__penalty': 'l1', 'logreg__solver': 'liblinear'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Hyperparameters for Logistic Regression Model\n",
    "gs_lr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "786bf664-67f6-47ba-bb94-8777cf7c1d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7035902146170281"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Cross-Validation Score for Random Forest Model\n",
    "gs_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fe21c11-2578-4f25-8253-7b91d826f8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7268742427732242"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best Cross-Validation Score for Logistic Regression Model\n",
    "gs_lr.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2b4f9-b0d0-4814-91c3-1eea4c0286e6",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae4d9045-2e29-4a54-b06d-fb443eeffe58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7291357501236662"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('countvec',CountVectorizer(min_df = 0.05, max_df = 0.95)),('ada',AdaBoostClassifier(random_state=42))]\n",
    "pipe_ada_booster = Pipeline(steps).fit(X_train_preprocessed,y_train)\n",
    "pipe_ada_booster.score(X_train_preprocessed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5775d5dd-d763-4784-bfb9-1bfcdc5ed2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7228819653159707"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_ada_booster = np.mean(cross_val_score(pipe_ada_booster, X_train_preprocessed,y_train,scoring='accuracy',cv=5))\n",
    "crossval_ada_booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a167743f-f541-4127-9564-d83bc3b5b316",
   "metadata": {},
   "source": [
    "### GradientBoosting Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "196bef58-49a3-47e5-88aa-3c705854f635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7461310154759381"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = ('gbc',GradientBoostingClassifier(random_state=42))\n",
    "steps[1] = gbc\n",
    "pipe_gbc = Pipeline(steps).fit(X_train_preprocessed,y_train)\n",
    "pipe_gbc.score(X_train_preprocessed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7715de87-e399-47d6-8efb-a43a74041f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7361320310125132"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_gradient_booster = np.mean(cross_val_score(pipe_gbc, X_train_preprocessed,y_train,scoring='accuracy',cv=5))\n",
    "crossval_gradient_booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9946d5a3-8d57-46b9-989f-badb7e1b449e",
   "metadata": {},
   "source": [
    "### XGBClassifier with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3476e469-8d75-4519-ab5f-38a34c7a6d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.829870680517278"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps = [('countvec',CountVectorizer(min_df = 0.05, max_df = 0.95)),('xgb',xgboost.XGBClassifier(random_state=42, objective='binary:logistic'))]\n",
    "xgb_pipe = Pipeline(steps).fit(X_train_preprocessed,y_train)\n",
    "xgb_pipe.score(X_train_preprocessed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2a106dc-91cc-469f-bb9a-c80db101474a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729489040068961"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossval_xg_booster = np.mean(cross_val_score(xgb_pipe, X_train_preprocessed,y_train,scoring='accuracy',cv=5))\n",
    "crossval_xg_booster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5b166-6e99-44ff-ac74-d39388863dc3",
   "metadata": {},
   "source": [
    "### Hyperparameter Grids for Boosting Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e8cdb3a-8421-46fd-9be5-337a5860f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ada = {'ada__n_estimators':[50,100,150,200],\n",
    "                  'ada__learning_rate':[0.01,0.1,0.2,0.5,1]\n",
    "                 }\n",
    "\n",
    "param_grid_gradient = {'gbc__n_estimators':[50,100,150,200],\n",
    "                       'gbc__learning_rate':[0.01,0.1,0.2,0.5,1],\n",
    "                       'gbc__max_depth':[1,2,3,4]\n",
    "                      }\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'xgb__n_estimators': [50, 100, 200],\n",
    "    'xgb__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'xgb__max_depth': [3, 5, 7, 9],\n",
    "    'xgb__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'xgb__colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'xgb__gamma': [0, 0.1, 0.2],\n",
    "    'xgb__min_child_weight': [1, 2, 3],\n",
    "    'xgb__reg_alpha': [0, 0.1, 0.5],\n",
    "    'xgb__reg_lambda': [1, 1.5, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de744de9-111b-42d3-b9be-671401ae6244",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_ada = GridSearchCV(estimator=pipe_ada_booster, param_grid=param_grid_ada,scoring='accuracy',cv=5,error_score=0)\n",
    "gs_gradient = GridSearchCV(estimator=pipe_gbc, param_grid=param_grid_gradient,scoring='accuracy',cv=5,error_score=0)\n",
    "gs_xgb = GridSearchCV(estimator=xgb_pipe, param_grid=param_grid_xgb,scoring='accuracy',cv=5,error_score=0)\n",
    "\n",
    "gs_ada.fit(X_train_preprocessed,y_train)\n",
    "gs_gradient.fit(X_train_preprocessed,y_train)\n",
    "gs_xgb.fit(X_train_preprocessed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e7d05-799f-494a-888a-14278253dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hyperparameters for AdaBoost Classifier\n",
    "gs_ada.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e39b6-6d0b-42e3-92ac-7e7e69f96dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Cross-Validation Score for AdaBoost Classifier\n",
    "gs_ada.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963cf67-f26a-425b-819d-d69e163c7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hyperparameters for GradientBoosting Classifier\n",
    "gs_gradient.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da983b8a-99f5-41f9-b65f-f2f1b9af4715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Cross-Validation Score for GradientBoosting Classifier\n",
    "gs_gradient.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ec180-e37e-47a1-af5d-7d0f3df2bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Hyperparameters for XGBClassifier \n",
    "gs_xgb.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d33c1a-3d6a-4fc4-a7c9-ab844a473183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Cross-Validation Score for XGBClassifier \n",
    "gs_xgb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb3dae-888c-4aae-8a02-8e561b4fd25c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
