{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d827980d-0c09-4b46-8cb0-3c9fc5f6f4dd",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "533803b6-91ec-4faa-a57e-4ca7192dc1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, recall_score,\\\n",
    "    accuracy_score, precision_score, f1_score, classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be5cb17-f005-465d-939b-f2babbb063b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                               text_  \n",
       "0  Love this!  Well made, sturdy, and very comfor...  \n",
       "1  love it, a great upgrade from the original.  I...  \n",
       "2  This pillow saved my back. I love the look and...  \n",
       "3  Missing information on how to use it, but it i...  \n",
       "4  Very nice set. Good quality. We have had the s...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fake reviews dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed7da3-dfe0-4207-b8f2-45d871c728a2",
   "metadata": {},
   "source": [
    "## Data Undersatding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ea703a-88c1-4ccb-b6a4-9376358321d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kindle_Store_5                  4730\n",
       "Books_5                         4370\n",
       "Pet_Supplies_5                  4254\n",
       "Home_and_Kitchen_5              4056\n",
       "Electronics_5                   3988\n",
       "Sports_and_Outdoors_5           3946\n",
       "Tools_and_Home_Improvement_5    3858\n",
       "Clothing_Shoes_and_Jewelry_5    3848\n",
       "Toys_and_Games_5                3794\n",
       "Movies_and_TV_5                 3588\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034dc82e-b462-48fd-b1f2-5438d3fa94d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40432.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.256579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.144354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rating\n",
       "count  40432.000000\n",
       "mean       4.256579\n",
       "std        1.144354\n",
       "min        1.000000\n",
       "25%        4.000000\n",
       "50%        5.000000\n",
       "75%        5.000000\n",
       "max        5.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab91030-1d16-4cce-8656-920c83fab120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = np.where(df['label'] == 'CG', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46dbda20-13ca-47a2-aa10-62d1b0379121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I had read some reviews saying that this bra r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I wasn't sure exactly what it would be. It is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>You can wear the hood by itself, wear it with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I liked nothing about this dress. The only rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I work in the wedding industry and have to wor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           category  rating label  \\\n",
       "0                Home_and_Kitchen_5     5.0    CG   \n",
       "1                Home_and_Kitchen_5     5.0    CG   \n",
       "2                Home_and_Kitchen_5     5.0    CG   \n",
       "3                Home_and_Kitchen_5     1.0    CG   \n",
       "4                Home_and_Kitchen_5     5.0    CG   \n",
       "...                             ...     ...   ...   \n",
       "40427  Clothing_Shoes_and_Jewelry_5     4.0    OR   \n",
       "40428  Clothing_Shoes_and_Jewelry_5     5.0    CG   \n",
       "40429  Clothing_Shoes_and_Jewelry_5     2.0    OR   \n",
       "40430  Clothing_Shoes_and_Jewelry_5     1.0    CG   \n",
       "40431  Clothing_Shoes_and_Jewelry_5     5.0    OR   \n",
       "\n",
       "                                                   text_  target  \n",
       "0      Love this!  Well made, sturdy, and very comfor...       1  \n",
       "1      love it, a great upgrade from the original.  I...       1  \n",
       "2      This pillow saved my back. I love the look and...       1  \n",
       "3      Missing information on how to use it, but it i...       1  \n",
       "4      Very nice set. Good quality. We have had the s...       1  \n",
       "...                                                  ...     ...  \n",
       "40427  I had read some reviews saying that this bra r...       0  \n",
       "40428  I wasn't sure exactly what it would be. It is ...       1  \n",
       "40429  You can wear the hood by itself, wear it with ...       0  \n",
       "40430  I liked nothing about this dress. The only rea...       1  \n",
       "40431  I work in the wedding industry and have to wor...       0  \n",
       "\n",
       "[40432 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17583586-ab05-4182-9acc-1d5e645fff49",
   "metadata": {},
   "source": [
    "## Text Preprocessing: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21d63507-d2a1-49ff-a5fc-879a191194c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the text data in the 'text_' column of df\n",
    "def tokenizer(x):\n",
    "    \n",
    "    corpus = [word_tokenize(doc) for doc in x]\n",
    "\n",
    "# getting common stop words in english that we'll remove during tokenization/text normalization\n",
    "    stop_words = stopwords.words('english')\n",
    "    corpus_no_stopwords = []\n",
    "    for words in corpus:\n",
    "        docs = [x.lower() for x in words if ((x.isalpha()) & (x not in stop_words))]\n",
    "        corpus_no_stopwords.append(docs)\n",
    "    return corpus_no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c671ab9-1944-48ff-96db-c374083e0f0c",
   "metadata": {},
   "source": [
    "## Lemmantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8703adb6-185e-474d-8d4d-6e9d4c702340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(corpus, as_string=True):\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "    lemmatized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        pos_tags = pos_tag(sentence)\n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in pos_tags:\n",
    "            pos = pos_tagger(tag)\n",
    "            if pos is not None:\n",
    "                lemmatized_word = lem.lemmatize(word, pos)\n",
    "            else:\n",
    "                lemmatized_word = lem.lemmatize(word)\n",
    "            lemmatized_sentence.append(lemmatized_word)\n",
    "        lemmatized_corpus.append(lemmatized_sentence)\n",
    "    if as_string:\n",
    "        lemmatized_corpus  = [' '.join(x) for x in lemmatized_corpus]\n",
    "    return lemmatized_corpus\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1206737e-503f-4251-94e1-1f0539cc12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = tokenizer(df['text_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "747e8fba-94e9-4fe4-b8e2-e8a68e842d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_corpus = lemmatizer(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a533175-9176-40cc-89f2-32503366188d",
   "metadata": {},
   "source": [
    "## Pre vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72435717-6b57-4004-96aa-9c3158368198",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_lemm_corpus = [' '.join(x) for x in lemmatized_corpus]\n",
    "df['text_preproccesed'] = pd.Series(data=lemmatized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb776e9b-7c6d-4c1a-a273-25cf41a0289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(min_df = 0.05, max_df = 0.95)\n",
    "X = vec.fit_transform(lemmatized_corpus)\n",
    "countvec_df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03accb0c-d1d8-4ff2-a841-dc49b9704b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 0.05, max_df = 0.95)\n",
    "Y = tfidf.fit_transform(lemmatized_corpus)\n",
    "tfidf_df = pd.DataFrame(Y.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14d7c444-29a6-4b89-80c6-d3c931eaf57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>anyone</th>\n",
       "      <th>best</th>\n",
       "      <th>big</th>\n",
       "      <th>bit</th>\n",
       "      <th>book</th>\n",
       "      <th>buy</th>\n",
       "      <th>ca</th>\n",
       "      <th>character</th>\n",
       "      <th>come</th>\n",
       "      <th>...</th>\n",
       "      <th>try</th>\n",
       "      <th>two</th>\n",
       "      <th>use</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115232</td>\n",
       "      <td>0.206830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061434</td>\n",
       "      <td>0.067497</td>\n",
       "      <td>0.252213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>0.064548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>0.179017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.110217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           also  anyone      best       big       bit  book      buy  \\\n",
       "0      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "1      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "2      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "3      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "4      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "...         ...     ...       ...       ...       ...   ...      ...   \n",
       "40427  0.000000     0.0  0.112657  0.000000  0.000000   0.0  0.00000   \n",
       "40428  0.000000     0.0  0.000000  0.091199  0.000000   0.0  0.13296   \n",
       "40429  0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "40430  0.064548     0.0  0.000000  0.000000  0.145785   0.0  0.11615   \n",
       "40431  0.179017     0.0  0.000000  0.000000  0.101079   0.0  0.00000   \n",
       "\n",
       "             ca  character      come  ...       try       two       use  \\\n",
       "0      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.436464   \n",
       "4      0.000000        0.0  0.000000  ...  0.000000  0.350925  0.000000   \n",
       "...         ...        ...       ...  ...       ...       ...       ...   \n",
       "40427  0.000000        0.0  0.103073  ...  0.115232  0.206830  0.000000   \n",
       "40428  0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "40429  0.000000        0.0  0.000000  ...  0.174608  0.000000  0.000000   \n",
       "40430  0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "40431  0.110217        0.0  0.000000  ...  0.000000  0.000000  0.151224   \n",
       "\n",
       "           want       way      well      work     would  write      year  \n",
       "0      0.000000  0.000000  0.365431  0.000000  0.000000    0.0  0.000000  \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.728227  \n",
       "2      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000  \n",
       "3      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000  \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000  \n",
       "...         ...       ...       ...       ...       ...    ...       ...  \n",
       "40427  0.099786  0.000000  0.075666  0.000000  0.077660    0.0  0.000000  \n",
       "40428  0.000000  0.000000  0.061434  0.067497  0.252213    0.0  0.000000  \n",
       "40429  0.000000  0.000000  0.114655  0.000000  0.117676    0.0  0.000000  \n",
       "40430  0.000000  0.000000  0.107335  0.000000  0.055082    0.0  0.000000  \n",
       "40431  0.000000  0.100946  0.000000  0.327058  0.000000    0.0  0.000000  \n",
       "\n",
       "[40432 rows x 89 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f42437c-3fb2-42df-b4b7-e087b539c05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40432x89 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 375538 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b6bd1dd-042d-4720-a383-cf3879867a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(df['text_'],df['target'], test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d46c6a-bc4b-4111-aa62-614373c80165",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = lemmatizer(tokenizer(X_train))\n",
    "X_test_preprocessed = lemmatizer(tokenizer(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22781fb0-adf4-4b27-a7c7-0acd8d3a9d82",
   "metadata": {},
   "source": [
    "### Try Random Forest and Logistic Regression with the Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62f4bc55-fb81-48a7-b673-5ced3b852b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(y_test, y_pred):\n",
    "    print(f\"\"\"    Accuracy: {round(accuracy_score(y_test,y_pred),3)}\n",
    "    \n",
    "    Recall: {round(recall_score(y_test,y_pred),3)}\n",
    "    \n",
    "    F1: {round(f1_score(y_test,y_pred),3)}\n",
    "    \n",
    "    Precision: {round(precision_score(y_test,y_pred),3)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9ac43cc-854f-4f7d-b718-bbed3b42b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('countvec',CountVectorizer(min_df = 0.05, max_df = 0.95)),('rfc',RandomForestClassifier(n_estimators=200))]\n",
    "pipe_cv_rf = Pipeline(steps)\n",
    "pipe_cv_rf.fit(X_train_preprocessed, y_train)\n",
    "y_pred_cv_rf = pipe_cv_rf.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fa1963f-6158-48f5-827f-754b14e7865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps[1] = ('logreg',LogisticRegression())\n",
    "pipe_cv_lr = Pipeline(steps)\n",
    "pipe_cv_lr.fit(X_train_preprocessed, y_train)\n",
    "y_pred_cv_lr = pipe_cv_lr.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57f45fd5-1e61-4bd5-941c-31f6fdfa2f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('tfidfvec',TfidfVectorizer(min_df = 0.05, max_df = 0.95)),('rfc',RandomForestClassifier(n_estimators=200))]\n",
    "pipe_idf_rf = Pipeline(steps)\n",
    "pipe_idf_rf.fit(X_train_preprocessed, y_train)\n",
    "y_pred_idf_rf = pipe_idf_rf.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08f98d58-25bd-4965-aecb-e126892a4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps[1] = ('logreg',LogisticRegression())\n",
    "pipe_idf_lr = Pipeline(steps)\n",
    "pipe_idf_lr.fit(X_train_preprocessed, y_train)\n",
    "y_pred_idf_lr = pipe_idf_lr.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97b5980c-fec0-46cd-b2b0-894fe428e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 0.768\n",
      "    \n",
      "    Recall: 0.755\n",
      "    \n",
      "    F1: 0.765\n",
      "    \n",
      "    Precision: 0.775\n"
     ]
    }
   ],
   "source": [
    "scores(y_test, y_pred_cv_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9322cd14-e78e-45b5-aa43-c183ad552513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 0.77\n",
      "    \n",
      "    Recall: 0.749\n",
      "    \n",
      "    F1: 0.765\n",
      "    \n",
      "    Precision: 0.782\n"
     ]
    }
   ],
   "source": [
    "scores(y_test,y_pred_idf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f04b21e-68ec-4cb7-bf1f-f190c934a2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 0.72\n",
      "    \n",
      "    Recall: 0.716\n",
      "    \n",
      "    F1: 0.719\n",
      "    \n",
      "    Precision: 0.722\n"
     ]
    }
   ],
   "source": [
    "scores(y_test,y_pred_cv_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "62d7f4ee-5971-4952-81f2-c78ae3e5f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Accuracy: 0.725\n",
      "    \n",
      "    Recall: 0.74\n",
      "    \n",
      "    F1: 0.729\n",
      "    \n",
      "    Precision: 0.718\n"
     ]
    }
   ],
   "source": [
    "scores(y_test,y_pred_idf_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada7ea6d-814b-4450-a644-8bb6904658cb",
   "metadata": {},
   "source": [
    "We run two models with both types of vectorizers, and based on our scores, we decided to stick with the TF-IDF vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b32b2c25-d9e0-4737-8c63-e7ef1e850c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_rf = [('countvec',CountVectorizer(min_df = 0.05, max_df = 0.95)),('rfc',RandomForestClassifier(n_estimators=200))]\n",
    "steps_lr = [('countvec',CountVectorizer(min_df = 0.05, max_df = 0.95)),('logreg',LogisticRegression())]\n",
    "\n",
    "pipe_rf = Pipeline(steps_rf)\n",
    "pipe_lr = Pipeline(steps_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a048d452-1a7f-481c-a84e-4c0a4dc7e657",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rf = {'rfc__n_estimators':[100,150,200],\n",
    "                       'rfc__max_depth':[2,3,4,5],\n",
    "                       'rfc__min_samples_leaf':[1,2,3]\n",
    "                      }\n",
    "\n",
    "param_lr = {'logreg__penalty':['l1', 'l2', 'elasticnet', None],\n",
    "                 'logreg__C':[0.001,0.01,0.1,1,10,100],\n",
    "                 'logreg__solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7bc7c3ea-486d-4c68-a28b-7893f9545264",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m gs_rf \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mpipe_rf, param_grid\u001b[38;5;241m=\u001b[39mparam_rf,scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      2\u001b[0m gs_lr \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mpipe_lr, param_grid\u001b[38;5;241m=\u001b[39mparam_lr,scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mgs_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_preprocessed\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m gs_lr\u001b[38;5;241m.\u001b[39mfit(X_train_preprocessed,y_train)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\validation.py:63\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     66\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     67\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args],\n\u001b[0;32m     68\u001b[0m                                  args[\u001b[38;5;241m-\u001b[39mextra_args:])]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    835\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    836\u001b[0m         all_candidate_params, n_splits, all_out,\n\u001b[0;32m    837\u001b[0m         all_more_results)\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 841\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    845\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1296\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1296\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\model_selection\\_search.py:795\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    793\u001b[0m               n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits))\n\u001b[1;32m--> 795\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m                   \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    813\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    814\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\utils\\fixes.py:222\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 222\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:598\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    596\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 598\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    600\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    602\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\pipeline.py:341\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03mFit all the transforms one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03m    This estimator\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    340\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 341\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    343\u001b[0m                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\pipeline.py:303\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    301\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 303\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    304\u001b[0m     cloned_transformer, X, y, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    305\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    306\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name])\n\u001b[0;32m    308\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\joblib\\memory.py:353\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\pipeline.py:754\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 754\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    756\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1202\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1199\u001b[0m min_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_df\n\u001b[0;32m   1200\u001b[0m max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_features\n\u001b[1;32m-> 1202\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1206\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1153\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1148\u001b[0m values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(values, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintc)\n\u001b[0;32m   1150\u001b[0m X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix((values, j_indices, indptr),\n\u001b[0;32m   1151\u001b[0m                   shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mlen\u001b[39m(indptr) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(vocabulary)),\n\u001b[0;32m   1152\u001b[0m                   dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m-> 1153\u001b[0m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vocabulary, X\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1163\u001b[0m, in \u001b[0;36m_cs_matrix.sort_indices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sort the indices of this matrix *in place*\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_sorted_indices:\n\u001b[1;32m-> 1163\u001b[0m     \u001b[43m_sparsetools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsr_sort_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_sorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gs_rf = GridSearchCV(estimator=pipe_rf, param_grid=param_rf,scoring='accuracy',cv=5,error_score=0)\n",
    "gs_lr = GridSearchCV(estimator=pipe_lr, param_grid=param_lr,scoring='accuracy',cv=5,error_score=0)\n",
    "gs_rf.fit(X_train_preprocessed,y_train)\n",
    "gs_lr.fit(X_train_preprocessed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b14f3f-cc27-4185-bd9d-b22c276b386f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
