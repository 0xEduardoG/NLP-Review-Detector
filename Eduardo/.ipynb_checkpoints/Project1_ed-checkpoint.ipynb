{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d827980d-0c09-4b46-8cb0-3c9fc5f6f4dd",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "533803b6-91ec-4faa-a57e-4ca7192dc1cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[99], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_confusion_matrix, recall_score,\\\n\u001b[0;32m     23\u001b[0m     accuracy_score, precision_score, f1_score, classification_report\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdaBoostClassifier, GradientBoostingClassifier\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize, word_tokenize, RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score,\\\n",
    "    accuracy_score, precision_score, f1_score, classification_report\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be5cb17-f005-465d-939b-f2babbb063b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             category  rating label  \\\n",
       "0  Home_and_Kitchen_5     5.0    CG   \n",
       "1  Home_and_Kitchen_5     5.0    CG   \n",
       "2  Home_and_Kitchen_5     5.0    CG   \n",
       "3  Home_and_Kitchen_5     1.0    CG   \n",
       "4  Home_and_Kitchen_5     5.0    CG   \n",
       "\n",
       "                                               text_  \n",
       "0  Love this!  Well made, sturdy, and very comfor...  \n",
       "1  love it, a great upgrade from the original.  I...  \n",
       "2  This pillow saved my back. I love the look and...  \n",
       "3  Missing information on how to use it, but it i...  \n",
       "4  Very nice set. Good quality. We have had the s...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fake reviews dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ed7da3-dfe0-4207-b8f2-45d871c728a2",
   "metadata": {},
   "source": [
    "## Data Undersatding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ea703a-88c1-4ccb-b6a4-9376358321d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kindle_Store_5                  4730\n",
       "Books_5                         4370\n",
       "Pet_Supplies_5                  4254\n",
       "Home_and_Kitchen_5              4056\n",
       "Electronics_5                   3988\n",
       "Sports_and_Outdoors_5           3946\n",
       "Tools_and_Home_Improvement_5    3858\n",
       "Clothing_Shoes_and_Jewelry_5    3848\n",
       "Toys_and_Games_5                3794\n",
       "Movies_and_TV_5                 3588\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034dc82e-b462-48fd-b1f2-5438d3fa94d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40432.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.256579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.144354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rating\n",
       "count  40432.000000\n",
       "mean       4.256579\n",
       "std        1.144354\n",
       "min        1.000000\n",
       "25%        4.000000\n",
       "50%        5.000000\n",
       "75%        5.000000\n",
       "max        5.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fab91030-1d16-4cce-8656-920c83fab120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = np.where(df['label'] == 'CG', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "46dbda20-13ca-47a2-aa10-62d1b0379121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "      <th>text_preproccesed</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>love well make sturdy comfortable i love very ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>love great upgrade original i mine couple year</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>this pillow save back i love look feel pillow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>miss information use great product price i</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>very nice set good quality we set two month</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I had read some reviews saying that this bra r...</td>\n",
       "      <td>i read review say bra run small i order two ba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I wasn't sure exactly what it would be. It is ...</td>\n",
       "      <td>i sure exactly would it little large small siz...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>You can wear the hood by itself, wear it with ...</td>\n",
       "      <td>you wear hood wear hood wear jacket without ho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I liked nothing about this dress. The only rea...</td>\n",
       "      <td>i like nothing dress the reason i give star i ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I work in the wedding industry and have to wor...</td>\n",
       "      <td>i work wed industry work long day foot outside...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           category  rating label  \\\n",
       "0                Home_and_Kitchen_5     5.0    CG   \n",
       "1                Home_and_Kitchen_5     5.0    CG   \n",
       "2                Home_and_Kitchen_5     5.0    CG   \n",
       "3                Home_and_Kitchen_5     1.0    CG   \n",
       "4                Home_and_Kitchen_5     5.0    CG   \n",
       "...                             ...     ...   ...   \n",
       "40427  Clothing_Shoes_and_Jewelry_5     4.0    OR   \n",
       "40428  Clothing_Shoes_and_Jewelry_5     5.0    CG   \n",
       "40429  Clothing_Shoes_and_Jewelry_5     2.0    OR   \n",
       "40430  Clothing_Shoes_and_Jewelry_5     1.0    CG   \n",
       "40431  Clothing_Shoes_and_Jewelry_5     5.0    OR   \n",
       "\n",
       "                                                   text_  \\\n",
       "0      Love this!  Well made, sturdy, and very comfor...   \n",
       "1      love it, a great upgrade from the original.  I...   \n",
       "2      This pillow saved my back. I love the look and...   \n",
       "3      Missing information on how to use it, but it i...   \n",
       "4      Very nice set. Good quality. We have had the s...   \n",
       "...                                                  ...   \n",
       "40427  I had read some reviews saying that this bra r...   \n",
       "40428  I wasn't sure exactly what it would be. It is ...   \n",
       "40429  You can wear the hood by itself, wear it with ...   \n",
       "40430  I liked nothing about this dress. The only rea...   \n",
       "40431  I work in the wedding industry and have to wor...   \n",
       "\n",
       "                                       text_preproccesed  target  \n",
       "0      love well make sturdy comfortable i love very ...       1  \n",
       "1         love great upgrade original i mine couple year       1  \n",
       "2          this pillow save back i love look feel pillow       1  \n",
       "3             miss information use great product price i       1  \n",
       "4            very nice set good quality we set two month       1  \n",
       "...                                                  ...     ...  \n",
       "40427  i read review say bra run small i order two ba...       0  \n",
       "40428  i sure exactly would it little large small siz...       1  \n",
       "40429  you wear hood wear hood wear jacket without ho...       0  \n",
       "40430  i like nothing dress the reason i give star i ...       1  \n",
       "40431  i work wed industry work long day foot outside...       0  \n",
       "\n",
       "[40432 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17583586-ab05-4182-9acc-1d5e645fff49",
   "metadata": {},
   "source": [
    "## Text Preprocessing: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21d63507-d2a1-49ff-a5fc-879a191194c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing the text data in the 'text_' column of df\n",
    "def tokenizer(x):\n",
    "    \n",
    "    corpus = [word_tokenize(doc) for doc in x]\n",
    "\n",
    "# getting common stop words in english that we'll remove during tokenization/text normalization\n",
    "    stop_words = stopwords.words('english')\n",
    "    corpus_no_stopwords = []\n",
    "    for words in corpus:\n",
    "        docs = [x.lower() for x in words if ((x.isalpha()) & (x not in stop_words))]\n",
    "        corpus_no_stopwords.append(docs)\n",
    "    return corpus_no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c671ab9-1944-48ff-96db-c374083e0f0c",
   "metadata": {},
   "source": [
    "## Lemmantizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8703adb6-185e-474d-8d4d-6e9d4c702340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(corpus, as_string=True):\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "    lemmatized_corpus = []\n",
    "    for sentence in corpus:\n",
    "        pos_tags = pos_tag(sentence)\n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in pos_tags:\n",
    "            pos = pos_tagger(tag)\n",
    "            if pos is not None:\n",
    "                lemmatized_word = lem.lemmatize(word, pos)\n",
    "            else:\n",
    "                lemmatized_word = lem.lemmatize(word)\n",
    "            lemmatized_sentence.append(lemmatized_word)\n",
    "        lemmatized_corpus.append(lemmatized_sentence)\n",
    "    if as_string:\n",
    "        lemmatized_corpus  = [' '.join(x) for x in lemmatized_corpus]\n",
    "    return lemmatized_corpus\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1206737e-503f-4251-94e1-1f0539cc12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = tokenizer(df['text_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "747e8fba-94e9-4fe4-b8e2-e8a68e842d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_corpus = lemmatizer(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a533175-9176-40cc-89f2-32503366188d",
   "metadata": {},
   "source": [
    "## Pre vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72435717-6b57-4004-96aa-9c3158368198",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_lemm_corpus = [' '.join(x) for x in lemmatized_corpus]\n",
    "df['text_preproccesed'] = pd.Series(data=lemmatized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fb776e9b-7c6d-4c1a-a273-25cf41a0289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(min_df = 0.05, max_df = 0.95)\n",
    "X = vec.fit_transform(lemmatized_corpus)\n",
    "countvec_df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "03accb0c-d1d8-4ff2-a841-dc49b9704b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df = 0.05, max_df = 0.95)\n",
    "Y = tfidf.fit_transform(lemmatized_corpus)\n",
    "tfidf_df = pd.DataFrame(Y.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "14d7c444-29a6-4b89-80c6-d3c931eaf57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>also</th>\n",
       "      <th>anyone</th>\n",
       "      <th>best</th>\n",
       "      <th>big</th>\n",
       "      <th>bit</th>\n",
       "      <th>book</th>\n",
       "      <th>buy</th>\n",
       "      <th>ca</th>\n",
       "      <th>character</th>\n",
       "      <th>come</th>\n",
       "      <th>...</th>\n",
       "      <th>try</th>\n",
       "      <th>two</th>\n",
       "      <th>use</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>write</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.365431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112657</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115232</td>\n",
       "      <td>0.206830</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.077660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.13296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061434</td>\n",
       "      <td>0.067497</td>\n",
       "      <td>0.252213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.174608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>0.064548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>0.179017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.110217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100946</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.327058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows × 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           also  anyone      best       big       bit  book      buy  \\\n",
       "0      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "1      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "2      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "3      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "4      0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "...         ...     ...       ...       ...       ...   ...      ...   \n",
       "40427  0.000000     0.0  0.112657  0.000000  0.000000   0.0  0.00000   \n",
       "40428  0.000000     0.0  0.000000  0.091199  0.000000   0.0  0.13296   \n",
       "40429  0.000000     0.0  0.000000  0.000000  0.000000   0.0  0.00000   \n",
       "40430  0.064548     0.0  0.000000  0.000000  0.145785   0.0  0.11615   \n",
       "40431  0.179017     0.0  0.000000  0.000000  0.101079   0.0  0.00000   \n",
       "\n",
       "             ca  character      come  ...       try       two       use  \\\n",
       "0      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3      0.000000        0.0  0.000000  ...  0.000000  0.000000  0.436464   \n",
       "4      0.000000        0.0  0.000000  ...  0.000000  0.350925  0.000000   \n",
       "...         ...        ...       ...  ...       ...       ...       ...   \n",
       "40427  0.000000        0.0  0.103073  ...  0.115232  0.206830  0.000000   \n",
       "40428  0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "40429  0.000000        0.0  0.000000  ...  0.174608  0.000000  0.000000   \n",
       "40430  0.000000        0.0  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "40431  0.110217        0.0  0.000000  ...  0.000000  0.000000  0.151224   \n",
       "\n",
       "           want       way      well      work     would  write      year  \n",
       "0      0.000000  0.000000  0.365431  0.000000  0.000000    0.0  0.000000  \n",
       "1      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.728227  \n",
       "2      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000  \n",
       "3      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000  \n",
       "4      0.000000  0.000000  0.000000  0.000000  0.000000    0.0  0.000000  \n",
       "...         ...       ...       ...       ...       ...    ...       ...  \n",
       "40427  0.099786  0.000000  0.075666  0.000000  0.077660    0.0  0.000000  \n",
       "40428  0.000000  0.000000  0.061434  0.067497  0.252213    0.0  0.000000  \n",
       "40429  0.000000  0.000000  0.114655  0.000000  0.117676    0.0  0.000000  \n",
       "40430  0.000000  0.000000  0.107335  0.000000  0.055082    0.0  0.000000  \n",
       "40431  0.000000  0.100946  0.000000  0.327058  0.000000    0.0  0.000000  \n",
       "\n",
       "[40432 rows x 89 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f42437c-3fb2-42df-b4b7-e087b539c05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40432x68 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 329640 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6b6bd1dd-042d-4720-a383-cf3879867a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(df['text_'],df['target'], test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53d46c6a-bc4b-4111-aa62-614373c80165",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = lemmatizer(tokenizer(X_train))\n",
    "X_test_preprocessed = lemmatizer(tokenizer(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f9ac43cc-854f-4f7d-b718-bbed3b42b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('countvec',CountVectorizer(min_df = 0.05, max_df = 0.95)),('rfc',RandomForestClassifier(n_estimators=200))]\n",
    "pipe = Pipeline(steps)\n",
    "pipe.fit(X_train_preprocessed, y_train)\n",
    "y_pred = pipe.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1963f-6158-48f5-827f-754b14e7865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('tfidfvec',TfidfVectorizer(min_df = 0.05, max_df = 0.95)),('rfc',RandomForestClassifier(n_estimators=200))]\n",
    "pipe = Pipeline(steps)\n",
    "pipe.fit(X_train_preprocessed, y_train)\n",
    "y_pred = pipe.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "57f45fd5-1e61-4bd5-941c-31f6fdfa2f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('tfidfvec',TfidfVectorizer(min_df = 0.05, max_df = 0.95)),('logreg',LogisticRegression())]\n",
    "pipe = Pipeline(steps)\n",
    "pipe.fit(X_train_preprocessed, y_train)\n",
    "y_pred = pipe.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f98d58-25bd-4965-aecb-e126892a4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('tfidfvec',TfidfVectorizer(min_df = 0.05, max_df = 0.95)),('logreg',LogisticRegression())]\n",
    "pipe = Pipeline(steps)\n",
    "pipe.fit(X_train_preprocessed, y_train)\n",
    "y_pred = pipe.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6389e2cd-30b0-4a3f-bd65-d448955bfa9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgboost' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m steps \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidfvec\u001b[39m\u001b[38;5;124m'\u001b[39m,TfidfVectorizer(min_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m, max_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.95\u001b[39m)),(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxgboost\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[43mxgboost\u001b[49m\u001b[38;5;241m.\u001b[39mXGBClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m'\u001b[39m))]\n\u001b[0;32m      2\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline(steps)\n\u001b[0;32m      3\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(X_train_preprocessed, y_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xgboost' is not defined"
     ]
    }
   ],
   "source": [
    "steps = [('tfidfvec',TfidfVectorizer(min_df = 0.05, max_df = 0.95)),('xgboost',xgboost.XGBClassifier(random_state=42, objective='binary:logistic'))]\n",
    "pipe = Pipeline(steps)\n",
    "pipe.fit(X_train_preprocessed, y_train)\n",
    "y_pred = pipe.predict(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "97b5980c-fec0-46cd-b2b0-894fe428e89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.7330609465440926,\n",
       "  'recall': 0.7100923482849604,\n",
       "  'f1-score': 0.7213938683196515,\n",
       "  'support': 6064},\n",
       " '1': {'precision': 0.7189897698209718,\n",
       "  'recall': 0.7415100560501154,\n",
       "  'f1-score': 0.7300762863171564,\n",
       "  'support': 6066},\n",
       " 'accuracy': 0.7258037922506183,\n",
       " 'macro avg': {'precision': 0.7260253581825322,\n",
       "  'recall': 0.725801202167538,\n",
       "  'f1-score': 0.7257350773184039,\n",
       "  'support': 12130},\n",
       " 'weighted avg': {'precision': 0.726024198151475,\n",
       "  'recall': 0.7258037922506183,\n",
       "  'f1-score': 0.7257357930989478,\n",
       "  'support': 12130}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test,y_pred,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d7f4ee-5971-4952-81f2-c78ae3e5f99e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4d52d-7b40-45d7-b86f-5176fe0f350c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048d452-1a7f-481c-a84e-4c0a4dc7e657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc7c3ea-486d-4c68-a28b-7893f9545264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aceb90f3-5f30-42c7-92c0-51e0c5078719",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattenedcorpus_tokens = pd.Series(list(itertools.chain(*lemmatized_corpus)))\n",
    "tokens_unique = pd.Series(flattenedcorpus_tokens.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6394d54f-244e-4fb7-9264-927bb1c0f9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               love\n",
       "1               well\n",
       "2               make\n",
       "3             sturdy\n",
       "4        comfortable\n",
       "            ...     \n",
       "31966    gallbladder\n",
       "31967        hippora\n",
       "31968         hyvent\n",
       "31969            dwr\n",
       "31970        merrels\n",
       "Length: 31971, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98eab3-fdcc-4c63-8fca-3feff9297fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990ab7a1-0906-46d2-9495-15613c6927c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17273570-6549-4ba7-83f0-f177b5d9d6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972b4ad-a4fe-4f2c-9ce1-c7ef28fc8282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423b2543-ec7d-40d2-b7d4-f8962c2f4f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e9261-0029-4651-a807-0fb6c4083cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    " AdaBoostClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b10542c-3ae2-4480-b56f-13a2426f8174",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad7ba992-fb65-4556-b695-99c1a6ab073d",
   "metadata": {},
   "source": [
    "### Dealing with Stop words + lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b597f3-ff88-487b-a201-a42fdd72694b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47953"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens_no_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3560f6c7-6fc0-4124-a81d-35074a2c2c1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'first_step_normalizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtok_norm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mfirst_step_normalizer\u001b[49m)\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'first_step_normalizer' is not defined"
     ]
    }
   ],
   "source": [
    "df['tok_norm'] = df['text_'].apply(first_step_normalizer)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469c572-5b25-4269-b3cb-1f69a35b8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_toks_flattened = pd.Series(list(\n",
    "    itertools.chain(*df['tok_norm'])))\n",
    "new_dictionary = norm_toks_flattened.unique()\n",
    "print(len(new_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d37a6b-8813-435d-9530-7e1c4054933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51fe415-1382-431f-94b9-28d5e1e74587",
   "metadata": {},
   "source": [
    "- Process removed 22.500 features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b86f15b-bc61-4d7f-a92e-de5226998736",
   "metadata": {},
   "source": [
    "## Text Preprocessing: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1c6357-4aa0-42e7-abd5-499dc453ac42",
   "metadata": {},
   "source": [
    "#### We created function which takes in untokenized document and returns fully normalized token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b98c0-cf85-42f3-a14c-6201c8321ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97728c8b-7f8e-493a-871d-ac2ca5afa8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc):\n",
    "\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "        \n",
    "    # remove stop words and punctuations, then lower case\n",
    "    doc_norm = [tok.lower() for tok in word_tokenize(doc) if ((tok.isalpha()) & (tok not in stop_words)) ]\n",
    "\n",
    "    # creates list of tuples with tokens and POS tags in wordnet format\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(doc_norm))) \n",
    "    doc_norm = [wnl.lemmatize(token, pos) for token, pos in wordnet_tagged if pos is not None]\n",
    "    \n",
    "    return doc_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aac37d0-1b5b-46af-8985-6c6e9379e5a3",
   "metadata": {},
   "source": [
    "### Applying text Tokenization/Normalization to whole body of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4024578d-fa1f-4db6-8d7d-cbbc6d186cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_normalized_corpus = df['text_'].apply(process_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b114d7-6d8c-41ec-8277-d1e6285c1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_normalized_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae9fd1-5eb7-4b3e-a91d-bd2cfc1b6840",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_fully_norm = pd.Series(list(itertools.chain(*fully_normalized_corpus)))\n",
    "len(flattened_fully_norm.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd0d37e-ed5c-4a78-a0dd-eedddeb09e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_fully_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7910a-c799-4f9a-8f17-210afb22c7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flattening the lists\n",
    "fnc_output = fully_normalized_corpus.apply(\" \".join)\n",
    "fnc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12cca8-477e-4485-8258-e208d99e54c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
